{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed6b12a"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Final Assignment\n",
        "## Deep Learning in Python\n",
        "#### Group 7: Sudjeeni Bonevacia, Mathilde ter Veen, Gigi Vissers"
      ],
      "id": "0ed6b12a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPkebMDR8dYN"
      },
      "source": [],
      "id": "mPkebMDR8dYN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91bb47fa"
      },
      "source": [
        "## Introduction to the notebook"
      ],
      "id": "91bb47fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327f0b62"
      },
      "source": [
        "The current computer vision project is concerned with training a convolutional neural network to detect lower-grade glioma brain tumors on MRI-scans. This type of brain tumor, although better manageable than their higher-grade counterparts, occurs often in younger adults and eventually progresses to a fatal stage with an average survival of 7 years after diagnosis (Claus et al., 2016). It is therefore of vital importance that they can be accurately detected from MRI images. Although the field of deep learning is not yet advanced enough to fully replace radiologists at this task, significant progress has been made in the field with classification CNN’s reaching accuracies of over 95% (Irmak, 2021; Li et al., 2021). Skilled radiologists reach accuracies of between 92 and 95% (Yan et al., 2016).\n",
        "\n",
        "The data used comes from 110 patients who had a lower-grade glioma brain tumor from The Cancer Genome Atlas (TCGA), and consists of images obtained from The Cancer Imaging Archive (TCIA) as well as manually created fluid-attenuated inversion recovery (FLAIR) masks. The FLAIR technique for MRI’s is meant to suppress darkening cerebrospinal fluid effects on the images, which brings out the contrast between the pixels. In total, there are 3929 images in the data set (Buda, Saha & Mazurowski, 2019).\n",
        "\n",
        "For this taks, we have started with the code of Anant Gupt (https://www.kaggle.com/anantgupt/brain-mri-detection-segmentation-resunet). We added advanced data augmentation techniques such as affine and elastic transforms, and tuned the hyperparameters to achieve higher accuracies.\n",
        "\n",
        "Besides merely spotting a tumor on an MRI, having algorithms that can identify type of tumor from its shape characteristics seems a viable path for the future as it can help save physicians considerable time for manual labelling, potentially reduces ambiguity when classifying the images, and most importantly increase the likelihood of successful treatment through early detection (Li et al., 2021; Sajjad et al., 2019).\n",
        "\n",
        "Therefore, as a special twist to our computer vision project, we decided to add a segmentation task on top of the classification task described above. To this end, we apply image segmentation techniques, which cluster the parts of the brain images into tumor or non-tumor areas. This happens at the pixel-level of the images to ensure that localization of the different classes can be realized (Ronneberger, Fischer & Brox, 2015). In order to evaluate the performance of this segmentation task we use the Dice coefficient, a metric of similarity between a predicted segmentation and the ground truth, most often used for this type of task in AI.\n",
        "\n",
        "The baseline code by Kaggle user Monkira (https://www.kaggle.com/monkira/brain-mri-segmentation-using-unet-keras) that we built on for this task uses a U-net architecture, which is often used in the literature for this type of medical classification problem. In this type of network, based on a fully convolutional neural network, pooling operations are replaced by upsampling operations (Ronneberger et al., 2015).\n"
      ],
      "id": "327f0b62"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0b3045"
      },
      "source": [
        "# 1. IMPORTING LIBRARIES AND DATASET"
      ],
      "id": "3a0b3045"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8b8ad61"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import cv2\n",
        "from skimage import io\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import random\n",
        "import glob\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from IPython.display import display"
      ],
      "id": "e8b8ad61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bAdz_4svbg3y",
        "outputId": "769a4e19-37a5-439f-9b48-c9da57da0c08"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3917b6ed-dbda-4f30-a858-b9238e7fa8c9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3917b6ed-dbda-4f30-a858-b9238e7fa8c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle (1).json to kaggle (1) (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1) (1).json': b'{\"username\":\"krishthakur351\",\"key\":\"a4d2e70ba6b869779c79de8f1a4c80f1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "id": "bAdz_4svbg3y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOhgrlR7bg87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c764df5c-2886-4874-9a60-fbd8e97cf95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle  -p\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "id": "YOhgrlR7bg87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PQ3ZawbhCA",
        "outputId": "974c322a-23ab-49d2-9dde-2c3515c21057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/goyaladi/fraud-detection-dataset\n",
            "License(s): CC0-1.0\n",
            "fraud-detection-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d goyaladi/fraud-detection-dataset\n"
      ],
      "id": "l4PQ3ZawbhCA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cbd83e"
      },
      "source": [
        "### 1.2 Importing the dataset\n",
        "\n",
        "In the dataset lgg-mri-segmentation we have the brain images and their corresponding masks (so the tumor location or no tumor) provided in .tif formats. Not every brain image has a corresponding mask, because it doesn't contain a tumor.\n",
        "In addition, we have a csv file with information about the 110 patients. The .csv file that contains for example age, ethnicity, death and the genomic clusters of patients. We will not be using this information in our analysis. In the graph below you can see that most of the patients are between the age of 30 and 60, so lower-grade glioma brain tumors often arise in young adults (Claus et al., 2015)."
      ],
      "id": "81cbd83e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "708dd4a6",
        "outputId": "2f7fd946-3446-43b7-db9f-faa6a34a630a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/fraud-detection-dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-57ba17ba113d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/fraud-detection-dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fraud-detection-dataset'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/fraud-detection-dataset\")\n",
        "df.head(10)"
      ],
      "id": "708dd4a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26eca305"
      },
      "outputs": [],
      "source": [
        "# This shows the first 6 rows of the patient data\n",
        "patientdata.head()"
      ],
      "id": "26eca305"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b228466"
      },
      "outputs": [],
      "source": [
        "# Make objects with the images and masks.\n",
        "brain_scans = []\n",
        "mask_files = glob.glob('/content/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n",
        "\n",
        "for i in mask_files:\n",
        "    print(i)\n",
        "    brain_scans.append(i.replace('_mask',''))\n",
        "\n",
        "print(brain_scans[:10])\n",
        "print(mask_files[:10])"
      ],
      "id": "5b228466"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66cf51b"
      },
      "source": [
        "### 1.3 Creating the final data frame:\n",
        "\n",
        "In this section we will combine the brain scans with their corresponding 'masks' in a dataframe. Next, we use a function to determine which masks contain a tumor and which masks are empty. The source for this function is: https://www.kaggle.com/shiveshchowdary/starter-image-segmentation.\n",
        "\n",
        "Then we apply the function to the masks in the dataframe and add a column with 0's and 1's, **where 0 indicate a mask with no tumor and 1 indicate a mask with a tumor.**\n",
        "This dataframe is needed for the classification task, because then the model can get the brain scans as input and train the column with the 0's and 1's as output."
      ],
      "id": "c66cf51b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fca1200b"
      },
      "outputs": [],
      "source": [
        "# Make a dataframe with the images and their corresponding masks\n",
        "data_img = pd.DataFrame({\n",
        "    \"image_path\":brain_scans,\n",
        "    \"mask_path\":mask_files\n",
        "})"
      ],
      "id": "fca1200b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e33bd26"
      },
      "outputs": [],
      "source": [
        "# Make a function that search for the largest pixel value in the masks, because that will indicate if the image have\n",
        "# a corresponding mask with a tumor or not.\n",
        "def positive_negative_diagnosis(file_masks):\n",
        "    mask = cv2.imread(file_masks)\n",
        "    value = np.max(mask)\n",
        "    if value > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Apply the function to the masks and return back a column with 1 and zeros, where 0 indicate no tumor and 1 a tumor.\n",
        "data_img[\"Tumor\"] = data_img[\"mask_path\"].apply(lambda x: positive_negative_diagnosis(x))"
      ],
      "id": "8e33bd26"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9fbdb22"
      },
      "outputs": [],
      "source": [
        "data_img # showing the data"
      ],
      "id": "e9fbdb22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b42321"
      },
      "source": [
        "# 2. DATA VISUALISATION"
      ],
      "id": "33b42321"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d7aab63"
      },
      "outputs": [],
      "source": [
        "# How many non-tumors (0) and tumors (1) are in the data\n",
        "data_img['Tumor'].value_counts()"
      ],
      "id": "3d7aab63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "543ed25d"
      },
      "outputs": [],
      "source": [
        "# Data vizualisation\n",
        "# SOURCE: https://www.kaggle.com/monkira/brain-mri-segmentation-using-unet-keras#Create-data-frame-and-split-data-on-train-set,-validation-set-and-test-set\n",
        "# https://www.kaggle.com/saivikassingamsetty/brain-tumor-segmentation-with-unet/notebook\n",
        "\n",
        "for i in range(1,40, 2):\n",
        "    img_path=brain_scans[i]\n",
        "    msk_path=mask_files[i]\n",
        "    img=cv2.imread(img_path)\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    msk=cv2.imread(msk_path)\n",
        "\n",
        "    #Plot the Brain MRI scans\n",
        "    original = img.copy()\n",
        "    fig, ax = plt.subplots(1,3,figsize = (15,5))\n",
        "    ax[0].imshow(original)\n",
        "    ax[0].set_title(\"Brain MRI\")\n",
        "\n",
        "    # Plot the corresponding mask\n",
        "    main = original.copy()\n",
        "    mask = msk.copy()\n",
        "    ax[1].imshow(mask)\n",
        "    ax[1].set_title(\"Tumor Location\")\n",
        "\n",
        "    # Plot the Brain MRI scan with their mask\n",
        "    main = original.copy()\n",
        "    label = cv2.imread(msk_path)\n",
        "    sample = np.array(np.squeeze(label), dtype = np.uint8)\n",
        "    contours, hier = cv2.findContours(sample[:,:,0],cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    sample_over_gt = cv2.drawContours(main, contours, -1,[255,0,0], thickness=-1)\n",
        "    ax[2].imshow(sample_over_gt)\n",
        "    ax[2].set_title(\"MRI Brain with highlighted Tumor\")"
      ],
      "id": "543ed25d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77cf2650"
      },
      "source": [
        "# 3. CREATING TEST/TRAIN/VALIDATION SET"
      ],
      "id": "77cf2650"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbbb23df"
      },
      "outputs": [],
      "source": [
        "# Convert the data in mask column to string format, to use categorical mode in flow_from_dataframe\n",
        "data_img['Tumor'] = data_img['Tumor'].apply(lambda x: str(x))\n",
        "data_img.info()"
      ],
      "id": "dbbb23df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c17c01f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data_img,test_size = 0.1)\n",
        "train, val = train_test_split(train,test_size = 0.2)\n",
        "print(train.values.shape)\n",
        "print(val.values.shape)\n",
        "print(test.values.shape)\n",
        "\n",
        "train.head()\n",
        "val.head()"
      ],
      "id": "c17c01f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c12fa7"
      },
      "source": [
        "### 3.1 Seeing how many tumors are in the train, validation and test set, respectively"
      ],
      "id": "86c12fa7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a31f2045"
      },
      "outputs": [],
      "source": [
        "#  SOURCE: adapted from https://www.kaggle.com/anantgupt/brain-mri-detection-segmentation-resunet#2:-DATA-VISUALIZATION\n",
        "\n",
        "import plotly.graph_objects as go  # using plotly to create interactive plots\n",
        "\n",
        "\n",
        "fig = go.Figure([go.Bar(x=train['Tumor'].value_counts().index,\n",
        "                        y=train['Tumor'].value_counts(),\n",
        "                        width=[.4, .4],\n",
        "                       )\n",
        "                ])\n",
        "fig.update_traces(marker_color=['darkolivegreen', 'firebrick'], opacity = 0.7\n",
        "                 )\n",
        "\n",
        "fig.update_layout(title_text=\"Tumor Count Train Set\",\n",
        "                  width=700,\n",
        "                  height=550,\n",
        "                  yaxis=dict(\n",
        "                             title_text=\"Count\",\n",
        "                             tickmode=\"array\",\n",
        "                             titlefont=dict(size=20)\n",
        "                           )\n",
        "                 )\n",
        "\n",
        "fig.update_yaxes(range = list([0,2000]))\n",
        "fig.update_xaxes(tick0 = 0, dtick = 1)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "fig2 = go.Figure([go.Bar(x=val['Tumor'].value_counts().index,\n",
        "                        y=val['Tumor'].value_counts(),\n",
        "                        width=[.4, .4]\n",
        "                       )\n",
        "                ])\n",
        "fig2.update_traces(marker_color=['darkolivegreen', 'firebrick'], opacity = 0.7\n",
        "                 )\n",
        "fig2.update_layout(title_text=\"Tumor Count Validation Set\",\n",
        "                  width=700,\n",
        "                  height=550,\n",
        "                  yaxis=dict(\n",
        "                             title_text=\"Count\",\n",
        "                             tickmode=\"array\",\n",
        "                             titlefont=dict(size=20)\n",
        "                           )\n",
        "                 )\n",
        "\n",
        "fig2.update_yaxes(range = list([0,2000]))\n",
        "fig2.update_xaxes(tick0 = 0, dtick = 1)\n",
        "\n",
        "\n",
        "fig2.show()\n",
        "\n",
        "fig3 = go.Figure([go.Bar(x=test['Tumor'].value_counts().index,\n",
        "                        y=test['Tumor'].value_counts(),\n",
        "                        width=[.4, .4]\n",
        "                       )\n",
        "                ])\n",
        "fig3.update_traces(marker_color=['darkolivegreen', 'firebrick'], opacity = 0.7\n",
        "                 )\n",
        "fig3.update_layout(title_text=\"Tumor Count Test Set\",\n",
        "                  width=700,\n",
        "                  height=550,\n",
        "                  yaxis=dict(\n",
        "                             title_text=\"Count\",\n",
        "                             tickmode=\"array\",\n",
        "                             titlefont=dict(size=20)\n",
        "                           )\n",
        "                 )\n",
        "\n",
        "fig3.update_yaxes(range = list([0,2000]))\n",
        "fig3.update_xaxes(tick0 = 0, dtick = 1)\n",
        "\n",
        "fig3.show()"
      ],
      "id": "a31f2045"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbb75f4a"
      },
      "source": [
        "# 4. CLASSIFICATION MODEL TO DETECT EXISTENCE TUMOR"
      ],
      "id": "dbb75f4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ede7d476"
      },
      "source": [
        "In this part of the notebook a convolutional neural network (CNN) approach will be introduced, including Data Augmentation to categorize brain MRI scan images into the existence of a tumor and not existence of a tumor.\n",
        "\n",
        "In some literature it was stated to add Canny Edge Detection to the process. We tried to do this ourselves, but in the way we did it (using cv2.Canny()), it didn't add something to the result.\n",
        "\n",
        "Transfer learning is used in most of the former build classification models (Kahn et al.,2020; Lundervold & Lundervold, 2019) and different pre-trained models are compared. In most cases the VGG-16 model gave the best results in the sense of accuracy. Expected accuracy based on literature could be somewhere between 92% and 96% (Simonyan & Zisserman, 2014; Kahn et al.,2020).\n",
        "\n",
        "The classification model can be summarized in the following picture, which is based on the process that is pictured in Kahn et al. (2020). The changes and the add ons that we made in the process are highlighted in the figure below and are further explained in the concerning steps:\n",
        "\n",
        "\n",
        "![Classificatiemodel.jpg](attachment:25b93b02-64c9-45d1-a48e-6ad68efd47ff.jpg)\n"
      ],
      "id": "ede7d476"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57672dee"
      },
      "source": [
        "### 4.1 Batch size\n",
        "\n",
        "In this step the batch size is defined. We did the tuning of the batchsize by trying out different batchsizes in the earlier stages of the coding process, before adding the data augmentation.\n",
        "\n",
        "The tuning is summarized in the following table:"
      ],
      "id": "57672dee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa65295a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Table\n",
        "fig, ax =plt.subplots(1,1)\n",
        "data=[[0.850, 0.919,  0.896]]\n",
        "column_labels=['Batchsize 16', 'Batchsize 32', 'Batchsize 64']\n",
        "row_label = ['Accuracy']\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "ax.table(cellText=data,colLabels=column_labels,rowLabels=row_label, loc=\"center\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#Barplot\n",
        "# Make a random dataset:\n",
        "height = [0.850, 0.919, 0.896]\n",
        "bars = ('16', '32', '64')\n",
        "x_pos = np.arange(len(bars))\n",
        "\n",
        "# Create bars and choose color\n",
        "plt.bar(x_pos, height, color = ['darkseagreen', 'seagreen', 'mediumseagreen'])\n",
        "\n",
        "# Add title and axis names\n",
        "plt.title('Accuracy per batchsize')\n",
        "plt.xlabel('Batchsize')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Create names on the x axis\n",
        "plt.xticks(x_pos, bars)\n",
        "\n",
        "# Show graph\n",
        "plt.show()"
      ],
      "id": "aa65295a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8c4630f"
      },
      "source": [
        "Based on this results we choose a batchsize of 32 because it gave the highest accuracy.\n",
        "\n",
        "A batchsize of 128 was too big and gave errors. Too large of a batch size will lead to poor generalization. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning (Kandel & Castelli, 2020)."
      ],
      "id": "a8c4630f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36392a59"
      },
      "source": [
        "### 4.2 Data augmentation\n",
        "A major challenge in brain tumor classification is a lack of available or useable data (Sajjad et al., 2019). Training deep neural networks often requires a vast amount of data to yield accurate classifications, which is why we have opted to include data augmentation to train on. In the area of computer vision, such data augmentation is a very common regularization technique used to combat overfitting on the training dataset, and hence increase performance on other data sets.\n",
        "\n",
        "Based on Nalepa, Marcinkiewicz and Kawulok (2019), we have elected several data augmentation techniques that have most often and most succesfully been used in similar tumor classification and segmentation tasks on the BraTS18 data set, which has been dubbed as a \"standard benchmark for validating existing and emerging brain-tumor detection and segmentation techniques\".\n",
        "\n",
        "We used affine and elastic transformations. With affine transformations, images are flipped, cropped, rotated, and zoomed to produce more images. They are often used in the literature, and can lead to significant increase training images.\n",
        "**Note:** This type of transform may lead to highly correlated images or anatomically incorrect images, so we have picked only those that we thought would decrease this possibility.\n",
        "- flipping (horizontal, because the left and right lobe of a brain are symmetrical looking),\n",
        "-  changes in width, height, and a sublte zoom, because not all brains on the MRI’s are of the same size, and height and width adjustments of the positions of the brain on the scan, so the model learns location invariance.\n",
        "\n",
        "Then elastic transformationsm, used for shape variations, are often mentioned in the literature as viable data augmentation technique in the medical field to learn context invariance when looking for abnormalities. It draws a gridline over images, and distorts the image along those lines (Nalepa et al., 2019; Isensee et al., 2018).\n",
        "\n",
        "\n",
        "We have used a function used in two notebooks (and many many online resources!) cited below, and tuned its parameters."
      ],
      "id": "36392a59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c99070db"
      },
      "outputs": [],
      "source": [
        "# SOURCE: https://www.kaggle.com/babbler/mnist-data-augmentation-with-elastic-distortion\n",
        "# https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n",
        "\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "\n",
        "\n",
        "def elastic_transform(image, alpha_range, sigma, random_state=None):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
        "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
        "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
        "       Proc. of the International Conference on Document Analysis and\n",
        "       Recognition, 2003.\n",
        "\n",
        "   # Arguments\n",
        "       image: Numpy array with shape (height, width, channels).\n",
        "       alpha_range: Float for fixed value or [lower, upper] for random value from uniform distribution.\n",
        "           Controls intensity of deformation.\n",
        "       sigma: Float, sigma of gaussian filter that smooths the displacement fields.\n",
        "       random_state: `numpy.random.RandomState` object for generating displacement fields.\n",
        "    \"\"\"\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState(None)\n",
        "\n",
        "    if np.isscalar(alpha_range):\n",
        "        alpha = alpha_range\n",
        "    else:\n",
        "        alpha = np.random.uniform(low=alpha_range[0], high=alpha_range[1])\n",
        "\n",
        "    shape = image.shape\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "\n",
        "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n",
        "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1)), np.reshape(z, (-1, 1))\n",
        "\n",
        "    transformed_images = map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
        "\n",
        "    return transformed_images"
      ],
      "id": "c99070db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898ae0a5"
      },
      "source": [
        "### 4.7 Evaluation of data augmentation settings\n",
        "\n",
        "We tuned the augmentation settings to try and find the highest accuracy whilst not overfitting too much.\n",
        "To this end, we have tried the following settings:\n",
        "\n",
        "\n",
        "*No augmentation*\n",
        "\n",
        "Test accuracy: 0.8830\n",
        "\n",
        "Test loss: 0.2641\n",
        "\n",
        "*All augmentation standard* (As often used in other notebooks and literature)\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = 0.2\n",
        "- height shift = 0.2\n",
        "- zoom range = [0.5, 0.75]\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range = 8\n",
        "- sigma = 3\n",
        "\n",
        "Test accuracy: 0.7917\n",
        "\n",
        "Test loss: 0.4504\n",
        "\n",
        "*All augmentations light*\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = 0.01\n",
        "- height shift = 0.01\n",
        "- zoom range [0.01,0.25]\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range: 0.25\n",
        "- sigma = 0.05\n",
        "\n",
        "Test accuracy: 0.6132\n",
        "\n",
        "Test loss: 0.8652\n",
        "\n",
        "*Only Affine augmentations standard*\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = 0.2\n",
        "- height shift = 0.2\n",
        "- zoom range [0.5,0.75]\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range: -\n",
        "- sigma = -\n",
        "\n",
        "Test accuracy: 0.7631\n",
        "\n",
        "Test loss: 0.4492\n",
        "\n",
        "*HF and Elastic augmentation*\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = -\n",
        "- height shift = -\n",
        "- zoom range = -\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range = 5\n",
        "- sigma = 2\n",
        "\n",
        "Test accuracy: 0.9005\n",
        "\n",
        "Test loss: 0.2381\n",
        "\n",
        "*HF and Elastic augmentation 2*\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = -\n",
        "- height shift = -\n",
        "- zoom range = -\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range = 0.5\n",
        "- sigma = 0.05\n",
        "\n",
        "Test accuracy: 0.8957\n",
        "\n",
        "Test loss: 0.2797\n",
        "\n",
        "*HF and Elastic augmentation 3*\n",
        "\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = -\n",
        "- height shift = -\n",
        "- zoom range = -\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range = [2,6]\n",
        "- sigma = 2.5\n",
        "\n",
        "Test accuracy: 0.9008\n",
        "\n",
        "Test loss: 0.2563\n",
        "\n",
        "*Just HF*\n",
        "Affine transforms:\n",
        "- horizontal flips = True\n",
        "- width shift = -\n",
        "- height shift = -\n",
        "- zoom range = -\n",
        "\n",
        "Elastic transforms:\n",
        "- alpha_range = -\n",
        "- sigma = -\n",
        "\n",
        "Test accuracy: 0.8193\n",
        "\n",
        "Test loss: 0.2848\n",
        "\n",
        "\n",
        "#### Conclusion:\n",
        "Using all augmentation techniques described before yielded rather low accuracies and high losses. The augmentation techniques that gave the best accuracies compared to the model trained on un-augmented data were horizontal flips and elastic transformations.\n",
        "This could be due to a number of reasons, including that tumors could occur on both hemispheres of the brain such that horizontal flips did not create images that the model was unlikley to recognize.\n",
        "The other affine transforms (zooms and shifts), although widely used in the literature, resulted in lower test accuracies. Cropping the image by zooming in might not have worked because a tumor is unlikely to be in the middle of the MRI, thus zooming in on this part does not teach the model much extra."
      ],
      "id": "898ae0a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506cf3f0"
      },
      "source": [
        "### Visualising the tuning"
      ],
      "id": "506cf3f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04626530"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracies: ###########################\n",
        "height = [0.8830, 0.7917, 0.6132, 0.7631, 0.9005, 0.8957, 0.9008, 0.8193]\n",
        "bars = ('No Augmentation', 'Full Augmentation standard values', 'Full Augmentation light values', 'Only Affine',\n",
        "        'Horizontal Flip + Elastic standard', 'Horizontal Flip + Elastic light', 'Horizontal flip + elastic range',\n",
        "        'Horizontal Flip'\n",
        "       )\n",
        "\n",
        "x_pos = np.arange(len(bars))\n",
        "\n",
        "colours = sns.color_palette(\"YlGnBu\", 8)\n",
        "\n",
        "# Create bars and choose color\n",
        "plt.bar(x_pos, height, color = colours)\n",
        "\n",
        "# Add title and axis names\n",
        "plt.title('Accuracy per Augmentation Setting')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Create names on the x axis\n",
        "plt.yticks((0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))\n",
        "plt.xticks(x_pos, bars, rotation = 90)\n",
        "plt.tick_params(top='off', bottom='on', left='on', right='off', labelleft='on', labelbottom='on')\n",
        "\n",
        "# Show graph\n",
        "plt.show()\n",
        "\n",
        "# Losses #########################\n",
        "# Accuracies:\n",
        "height = [0.2641, 0.4504, 0.8652, 0.4492, 0.2381, 0.2797, 0.2563, 0.2848]\n",
        "bars = ('No Augmentation', 'Full Augmentation standard values', 'Full Augmentation light values', 'Only Affine',\n",
        "        'Horizontal Flip + Elastic standard', 'Horizontal Flip + Elastic light', 'Horizontal flip + elastic range',\n",
        "        'Horizontal Flip'\n",
        "       )\n",
        "\n",
        "colours = sns.color_palette(\"YlGnBu\", 8)\n",
        "\n",
        "# Create bars and choose color\n",
        "plt.bar(x_pos, height, color = colours)\n",
        "\n",
        "# Add title and axis names\n",
        "plt.title('Loss per Augmentation Setting')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Create names on the x axis\n",
        "plt.yticks((0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))\n",
        "plt.xticks(x_pos, bars, rotation = 90)\n",
        "plt.tick_params(top='off', bottom='on', left='on', right='off', labelleft='on', labelbottom='on')\n",
        "\n",
        "# Show graph\n",
        "plt.show()"
      ],
      "id": "04626530"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c69553"
      },
      "source": [
        "### Adding the data augmentation to the image data generator"
      ],
      "id": "89c69553"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5HQ8UhJfG7J"
      },
      "outputs": [],
      "source": [
        "!pip install  keras-preprocessing"
      ],
      "id": "K5HQ8UhJfG7J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c398f690"
      },
      "outputs": [],
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255.,\n",
        "                             validation_split=0.15,\n",
        "                             horizontal_flip = True,\n",
        "                            # zoom_range = ,\n",
        "                            # width_shift_range= ,\n",
        "                            # height_shift_range= ,\n",
        "                             preprocessing_function = lambda x: elastic_transform(x,\n",
        "                                                                                 alpha_range =  [2,6],\n",
        "                                                                                 sigma = 2.5)\n",
        "                            )\n",
        "\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(train,\n",
        "                                              directory='./',\n",
        "                                              x_col='image_path',\n",
        "                                              y_col='Tumor',\n",
        "                                              subset='training',\n",
        "                                              class_mode='binary',\n",
        "                                              batch_size=32,\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(256,256)\n",
        "                                             )\n",
        "\n",
        "valid_generator = datagen.flow_from_dataframe(val,\n",
        "                                              directory='./',\n",
        "                                              x_col='image_path',\n",
        "                                              y_col='Tumor',\n",
        "                                              subset='validation',\n",
        "                                              class_mode='binary',\n",
        "                                              batch_size=32,\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(256,256)\n",
        "                                             )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(test,\n",
        "                                                  directory='./',\n",
        "                                                  x_col='image_path',\n",
        "                                                  y_col='Tumor',\n",
        "                                                  class_mode='binary',\n",
        "                                                  batch_size=32,\n",
        "                                                  shuffle=False,\n",
        "                                                  target_size=(256,256)\n",
        "                                                 )"
      ],
      "id": "c398f690"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea55e127"
      },
      "source": [
        "### 4.3 Define pretrained base\n",
        "\n",
        "As noted down above, the VGG-16 model as pretrained base, should give us the best results based on literature. The pretrained base is loaded below.\n"
      ],
      "id": "ea55e127"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2178dfb"
      },
      "outputs": [],
      "source": [
        "# Taking a pretrained base\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "pretrained_base = VGG16(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(256,256,3)))\n",
        "pretrained_base.summary()\n",
        "\n",
        "# The pretrained database not trainable.\n",
        "pretrained_base.trainable = False"
      ],
      "id": "a2178dfb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61b1694c"
      },
      "source": [
        "### 4.4 Attach head\n",
        "\n",
        "In this section we define the classifier head and attach this to the pretrained base.\n",
        "\n",
        "In the classifier head, the Flatten layer transforms the two dimensional outputs of the base into the one dimensional inputs needed by the head.\n",
        "\n",
        "The drop out layer omits some of the neurons at each step from the layer making the neurons more independent from the neighbouring neurons. It helps in avoiding overfitting. We tried different drop-out parameters, between 0.2 and 0.5, but the parameters as used in our example code were given the best results.\n",
        "\n",
        "The Dense layer is the output layer which classifies the image into 1 of the 2 possible classes. The sigmoid function is used for the two-class logistic regression, instead softmax function as was used in the notebook that provided us the example code. The softmax function should be used for the multiclass logistic regression.\n",
        "\n",
        "The classifier head is based on the accuracy results in other notebooks:\n",
        "\n",
        "https://www.kaggle.com/anantgupt/brain-mri-detection-segmentation-resunet#7:-CLASSIFIACTION-MODEL-EVALUATION\n",
        "https://www.kaggle.com/jaykumar1607/brain-tumor-mri-classification-tensorflow-cnn"
      ],
      "id": "61b1694c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eacf25e7"
      },
      "outputs": [],
      "source": [
        "# Attaching head\n",
        "from tensorflow.keras import layers\n",
        "#from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "model = keras.Sequential([\n",
        "\n",
        "    # Base\n",
        "    pretrained_base,\n",
        "\n",
        "    # Head\n",
        "    layers.Flatten(name='Flatten'),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "id": "eacf25e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ed05450"
      },
      "source": [
        "### 4.5 Train\n",
        "\n",
        "In defining the loss function, all of the example notebooks that we used, use the categorical_crossentropy, instead of the binary_crossentropy. This strange, because we only have two target classes. With the original code out of that notebooks, we ran the code with the categorical_crossentropy and it gave a much higher accuracy, than with the use of the binary_crossentropy (0.898 compared to 0.628).\n",
        "\n",
        "This was peculiar because the formulas for both are not so much different:\n",
        "![download (1).png](attachment:9784585a-959b-4f55-a6c8-d66dd7970eeb.png)\n",
        "\n",
        "Source: https://newbedev.com/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances-for-the-same-problem.\n",
        "\n",
        "The solution of this problem is in the coding of the head. The wrong activation function was used. Instead of the softmax-function, the sigmoid function needed to be used with a scalar target and not a one-hot encoded target.\n",
        "\n",
        "As an optimizer Adam is used. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning. Adam is a great general-purpose optimizer and we found it used in all sources we found.\n"
      ],
      "id": "4ed05450"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98db3651"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics= ['accuracy']\n",
        "             )\n",
        "\n",
        "model.summary()"
      ],
      "id": "98db3651"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45c0de7d"
      },
      "source": [
        "We used the following callbacks:\n",
        "\n",
        "EarlyStopping helps us to stop the training of the model early if there is no increase in the parameter. It is a form of regulation to avoid overfitting. We have tried different minimal delta's in the script, between 0.001 and 0.0001. Also looking at the performance of the script, 0.001 gave us the best results.\n",
        "\n",
        "ModelCheckpoint helps us to save the model by monitoring a specific parameter of the model. In this case, We monitoring validation accuracy by passing val_acc to ModelCheckpoint.\n",
        "\n",
        "ReduceLROnPlateau helps us reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced (https://keras.io/api/callbacks/reduce_lr_on_plateau/)."
      ],
      "id": "45c0de7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e09bb38"
      },
      "outputs": [],
      "source": [
        "earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0.001,\n",
        "                              mode='min',\n",
        "                              verbose=1,\n",
        "                              patience=20,\n",
        "                              restore_best_weights = True\n",
        "                             )\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=\"vgg16_1.h5\",\n",
        "                               monitor = 'val_accuracy',\n",
        "                               verbose=1,\n",
        "                               save_best_only=True\n",
        "                              )\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              mode='min',\n",
        "                              verbose=1,\n",
        "                              patience=15,\n",
        "                              min_delta=0.0001,\n",
        "                              factor=0.2\n",
        "                             )\n",
        "\n",
        "callbacks = [checkpointer, earlystopping, reduce_lr]"
      ],
      "id": "7e09bb38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcf8a506"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "              steps_per_epoch= train_generator.n // train_generator.batch_size,\n",
        "              epochs = 5,\n",
        "              validation_data= valid_generator,\n",
        "              validation_steps= valid_generator.n // valid_generator.batch_size,\n",
        "              callbacks=callbacks)"
      ],
      "id": "dcf8a506"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4bb4137"
      },
      "source": [
        "### 4.6 Classification Model Evaluation"
      ],
      "id": "a4bb4137"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "499af0f7"
      },
      "outputs": [],
      "source": [
        "history.history.keys()"
      ],
      "id": "499af0f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "329821a5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss']);\n",
        "plt.plot(history.history['val_loss']);\n",
        "plt.title(\"Classification Model LOSS\");\n",
        "plt.ylabel(\"loss\");\n",
        "plt.xlabel(\"Epochs\");\n",
        "plt.legend(['train', 'val']);\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy']);\n",
        "plt.plot(history.history['val_accuracy']);\n",
        "plt.title(\"Classification Model Acc\");\n",
        "plt.ylabel(\"Accuracy\");\n",
        "plt.xlabel(\"Epochs\");\n",
        "plt.legend(['train', 'val']);"
      ],
      "id": "329821a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12f6e878"
      },
      "source": [
        "### Test accuracy:"
      ],
      "id": "12f6e878"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5464a78"
      },
      "outputs": [],
      "source": [
        "_, acc = model.evaluate(test_generator)\n",
        "print(\"Test accuracy : {} %\".format(acc*100))"
      ],
      "id": "b5464a78"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8afa465b"
      },
      "source": [
        "\n",
        "### Literature\n",
        "Buda, M., Saha, A., & Mazurowski, M. A. (2019). Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm. *Computers in biology and medicine*, 109, 218-225.\n",
        "\n",
        "Claus, E. B., Walsh, K. M., Wiencke, J. K., Molinaro, A. M., Wiemels, J. L., Schildkraut, J. M., ... & Wrensch, M. (2015). Survival and low-grade glioma: the emergence of genetic information. Neurosurgical focus, 38(1), E6.\n",
        "\n",
        "Irmak, E. (2021). Multi-Classification of Brain Tumor MRI Images Using Deep Convolutional Neural Network with Fully Optimized Framework. Iranian Journal of Science and Technology, Transactions of Electrical Engineering, 1-22.\n",
        "\n",
        "Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., & Maier-Hein, K. H. (2018). No new-net. In International MICCAI Brainlesion Workshop (pp. 234-244). Springer, Cham.\n",
        "\n",
        "Kandel, I., Castelli, M. (2020). The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset, ICT Express, Volume 6, Issue 4, 312-315. https://doi.org/10.1016/j.icte.2020.04.010.\n",
        "\n",
        "Li, CC., Wu, MY., Sun, YC. (2021). Ensemble classification and segmentation for intracranial metastatic tumors on MRI images based on 2D U-nets. Sci Rep 11, 20634. https://doi.org/10.1038/s41598-021-99984-5\n",
        "\n",
        "Lundervold, A. S., & Lundervold, A. (2019). An overview of deep learning in medical imaging focusing on MRI. Zeitschrift für Medizinische Physik, 29(2), 102-127. https://doi.org/10.1016/j.zemedi.2018.11.002\n",
        "\n",
        "Nalepa, J., Marcinkiewicz, M., & Kawulok, M. (2019). Data augmentation for brain-tumor segmentation: a review. Frontiers in computational neuroscience, 13, 83.\n",
        "\n",
        "Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.\n",
        "\n",
        "Sajjad, M., Khan, S., Muhammad, K., Wu, W., Ullah, A., & Baik, S. W. (2019). Multi-grade brain tumor classification using deep CNN with extensive data augmentation. Journal of computational science, 30, 174-182.\n",
        "\n",
        "Seetha, J., & Raja, S. S. (2018). Brain tumor classification using convolutional neural networks. Biomedical & Pharmacology Journal, 11(3), 1457.\n",
        "\n",
        "Simonyan,  K.,  Zisserman,  K.  (2014).  Very  deep convolutional  networks for  large-scale  image recognition.  CoRR, vol. abs/1409.1556.  "
      ],
      "id": "8afa465b"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 16145.755352,
      "end_time": "2021-12-23T18:11:41.989710",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-12-23T13:42:36.234358",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}